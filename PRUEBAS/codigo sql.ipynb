{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dcaf3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\maria\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\maria\\anaconda3\\lib\\site-packages (2.0.39)\n",
      "Collecting pymysql\n",
      "  Downloading pymysql-1.1.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from sqlalchemy) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from sqlalchemy) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pymysql-1.1.2-py3-none-any.whl (45 kB)\n",
      "Installing collected packages: pymysql\n",
      "Successfully installed pymysql-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas sqlalchemy pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c095b209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Conexi√≥n exitosa a la base de datos.\n",
      "   Tabla 'ratings' eliminada para permitir el borrado en cascada.\n",
      "   Tabla 'compensation' eliminada para permitir el borrado en cascada.\n",
      "   Tabla 'background' eliminada para permitir el borrado en cascada.\n",
      "‚úÖ CSV le√≠do. Total de filas: 1614\n",
      "\n",
      "‚è≥ Ajustando tipos de datos y precisi√≥n...\n",
      "‚úÖ Ajustes de tipos completados.\n",
      "\n",
      "‚öôÔ∏è Exportando tabla: job_roles...\n",
      "‚úÖ Tabla 'job_roles' exportada con √©xito.\n",
      "\n",
      "‚öôÔ∏è Exportando tabla: employees...\n",
      "‚úÖ Tabla 'employees' exportada con √©xito.\n",
      "\n",
      "‚öôÔ∏è Exportando tabla: background...\n",
      "‚úÖ Tabla 'background' exportada con √©xito.\n",
      "\n",
      "‚öôÔ∏è Exportando tabla: compensation...\n",
      "‚úÖ Tabla 'compensation' exportada con √©xito.\n",
      "\n",
      "‚öôÔ∏è Exportando tabla: ratings...\n",
      "‚úÖ Tabla 'ratings' exportada con √©xito.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import numpy as np\n",
    "\n",
    "db_user = 'root'\n",
    "db_password = 'AlumnaAdalab' \n",
    "db_host = '127.0.0.1'\n",
    "database_name = 'Talento_ABC_Corporation' \n",
    "NOMBRE_ESQUEMA = 'Talento_ABC_Corporation' \n",
    "NOMBRE_CSV = 'hr_data_clean.csv' \n",
    "\n",
    "CADENA_CONEXION = f'mysql+pymysql://{db_user}:{db_password}@{db_host}:3306/{database_name}'\n",
    "\n",
    "\n",
    "def normalizar_y_exportar_a_sql(archivo_csv, esquema, cadena_conexion):\n",
    "    \n",
    "  \n",
    "    try:\n",
    "        engine = create_engine(cadena_conexion)\n",
    "        print(\"üîå Conexi√≥n exitosa a la base de datos.\")\n",
    "        \n",
    "        # --- PASO CLAVE: ELIMINAR TABLAS HIJAS PARA EVITAR ERROR DE FK ---\n",
    "        # Borramos las tablas dependientes (ratings, compensation, background) para que 'employees' pueda borrarse.\n",
    "        tablas_a_dropear_manualmente = ['ratings', 'compensation', 'background']\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            for tabla in tablas_a_dropear_manualmente:\n",
    "                try:\n",
    "                    # Ejecutar DROP TABLE IF EXISTS\n",
    "                    connection.execute(text(f\"DROP TABLE IF EXISTS {tabla};\"))\n",
    "                    connection.commit()\n",
    "                    print(f\"   Tabla '{tabla}' eliminada para permitir el borrado en cascada.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è Advertencia: No se pudo eliminar la tabla {tabla}. {e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al conectar o preparar la BD: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Leer el CSV y preparar el DataFrame principal\n",
    "    try:\n",
    "        df_principal = pd.read_csv(archivo_csv)\n",
    "        df_principal.columns = df_principal.columns.str.lower().str.replace('[^0-9a-zA-Z]+', '_', regex=True)\n",
    "        print(f\"‚úÖ CSV le√≠do. Total de filas: {len(df_principal)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: Archivo CSV no encontrado en '{archivo_csv}'\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    ## üîß 3. AJUSTE DE TIPOS DE DATOS (Para asegurar la precisi√≥n SQL)\n",
    "    # ----------------------------------------------------------------------\n",
    "    print(\"\\n‚è≥ Ajustando tipos de datos y precisi√≥n...\")\n",
    "    \n",
    "    # --- A. Fechas ---\n",
    "    df_principal['datebirth'] = pd.to_datetime(df_principal['datebirth'], errors='coerce')\n",
    "\n",
    "    # --- B. Decimales (Redondeo a 2 decimales para coincidir con DECIMAL(X, 2)) ---\n",
    "    decimal_cols = [\n",
    "        'totalworkingyears', 'yearssincelastpromotion', \n",
    "        'dailyrate_', 'hourlyrate_', 'monthlyincome_', 'monthlyrate_', \n",
    "        'salary_', 'percentsalaryhike', 'sameasmonthlyincome_'\n",
    "    ]\n",
    "    for col in decimal_cols:\n",
    "        if col in df_principal.columns:\n",
    "            df_principal[col] = pd.to_numeric(df_principal[col], errors='coerce').round(2)\n",
    "    \n",
    "    # --- C. Enteros Clave ---\n",
    "    integer_cols = ['employeenumber', 'numberchildren', 'joblevel', 'education', 'trainingtimeslastyear', 'stockoptionlevel']\n",
    "    for col in integer_cols:\n",
    "        if col in df_principal.columns:\n",
    "            df_principal[col] = pd.to_numeric(df_principal[col], errors='coerce').fillna(0).astype(pd.Int64Dtype())\n",
    "\n",
    "    print(\"‚úÖ Ajustes de tipos completados.\")\n",
    "    \n",
    "    # ----------------------------------------------------------------------\n",
    "    ## üß© 4. NORMALIZACI√ìN: Crear Claves For√°neas y Tablas\n",
    "    # ----------------------------------------------------------------------\n",
    "    \n",
    "    # --- A. Normalizar job_roles (Tabla de B√∫squeda) ---\n",
    "    # La columna 'roledepartamentnumber' no existe en el CSV, as√≠ que no se selecciona.\n",
    "    df_job_roles = df_principal[[\n",
    "        'jobrole', 'department', 'joblevel' \n",
    "    ]].drop_duplicates().reset_index(drop=True).copy()\n",
    "    \n",
    "    # Creaci√≥n de la Clave Primaria (PK) artificial: job_role_id\n",
    "    df_job_roles['job_role_id'] = (df_job_roles.index + 1).astype(int)\n",
    "    \n",
    "    # --- B. FUSIONAR IDs: Asignar la FK (job_role_id) al DataFrame Principal ---\n",
    "    df_principal = pd.merge(\n",
    "        df_principal,\n",
    "        df_job_roles[['jobrole', 'job_role_id']],\n",
    "        on='jobrole',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # --- C. Definir los DataFrames por Tabla (Seleccionar columnas exactas) ---\n",
    "    \n",
    "    # 1. Tabla EMPLOYEES (Tabla Padre Principal)\n",
    "    columnas_employees = [\n",
    "        'employeenumber', 'gender', 'maritalstatus', 'datebirth', 'numberchildren', 'distancefromhome', \n",
    "        'numcompaniesworked', 'totalworkingyears', 'yearsatcompany', 'yearsincurrentrole', \n",
    "        'yearssincelastpromotion', 'yearswithcurrmanager', 'attrition', 'overtime', \n",
    "        'standardhours', 'remotework', 'education', 'job_role_id'\n",
    "    ]\n",
    "    df_employees = df_principal[columnas_employees].copy()\n",
    "    # Mapeo de nombres de columna:\n",
    "    df_employees = df_employees.rename(columns={'education': 'education_id', 'numberchildren': 'children'})\n",
    "    \n",
    "    # 2. Tabla BACKGROUND\n",
    "    columnas_background = [\n",
    "        'employeenumber', 'education', 'educationfield', 'trainingtimeslastyear',\n",
    "        'numcompaniesworked', 'totalworkingyears'\n",
    "    ]\n",
    "    df_background = df_principal[columnas_background].copy()\n",
    "    \n",
    "    # 3. Tabla COMPENSATION\n",
    "    columnas_compensation = [\n",
    "        'employeenumber', 'dailyrate_', 'hourlyrate_', 'monthlyincome_',\n",
    "        'monthlyrate_', 'salary_', 'percentsalaryhike', 'stockoptionlevel',\n",
    "        'sameasmonthlyincome_'\n",
    "    ]\n",
    "    df_compensation = df_principal[columnas_compensation].copy()\n",
    "    # Renombrar para quitar el '_' y coincidir con el SQL\n",
    "    df_compensation.columns = [col.replace('_', '') if col.endswith('_') else col for col in df_compensation.columns]\n",
    "\n",
    "\n",
    "    # 4. Tabla RATINGS\n",
    "    columnas_ratings = [\n",
    "        'employeenumber', 'environmentsatisfaction', 'jobinvolvement',\n",
    "        'jobsatisfaction', 'relationshipsatisfaction', 'performancerating',\n",
    "        'worklifebalance'\n",
    "    ]\n",
    "    df_ratings = df_principal[columnas_ratings].copy()\n",
    "    \n",
    "    # ----------------------------------------------------------------------\n",
    "    ## üì§ 5. EXPORTACI√ìN a SQL \n",
    "    # ----------------------------------------------------------------------\n",
    "    \n",
    "    # El orden de la lista es crucial para la CREACI√ìN: Padre primero.\n",
    "    tablas_a_exportar = [\n",
    "        (df_job_roles, 'job_roles'), \n",
    "        (df_employees, 'employees'), \n",
    "        (df_background, 'background'), \n",
    "        (df_compensation, 'compensation'), \n",
    "        (df_ratings, 'ratings') \n",
    "    ]\n",
    "    \n",
    "    for df_a_exportar, nombre_tabla in tablas_a_exportar:\n",
    "        print(f\"\\n‚öôÔ∏è Exportando tabla: {nombre_tabla}...\")\n",
    "        \n",
    "        df_a_exportar.to_sql(\n",
    "            name=nombre_tabla,\n",
    "            con=engine,\n",
    "            if_exists='replace', # Forzar la recreaci√≥n de la tabla\n",
    "            index=False \n",
    "        )\n",
    "        print(f\"‚úÖ Tabla '{nombre_tabla}' exportada con √©xito.\")\n",
    "\n",
    "\n",
    "# --- EJECUCI√ìN DEL SCRIPT ---\n",
    "normalizar_y_exportar_a_sql(NOMBRE_CSV, NOMBRE_ESQUEMA, CADENA_CONEXION)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
